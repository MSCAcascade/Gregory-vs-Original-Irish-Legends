{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164d6c9e-8dfa-4186-847a-116124e1360e",
   "metadata": {},
   "source": [
    "**Note:** Each keyword/file name/etc. has either \"g_\" or \"o_\" in the name. \"g_\" stands for Lady Gregory's texts and \"o_\" stands for the Original texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ee818-1637-48f7-b78a-13577d45b5ed",
   "metadata": {},
   "source": [
    "## Packages/Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c5e0e-aa9b-4db1-a1cb-c065e0992966",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a3d84-819d-49cf-a869-cd6e138c3e8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c95ea-e6f3-4787-addc-2481e4c19441",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# vader\n",
    "nltk.download('vader_lexicon')  \n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c74524-633e-4747-9eae-77db5e0b3af2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TFBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53e08f-4595-4837-8d9c-931caf28e829",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "#!pip install pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff8f45-94b6-4859-a5d9-e5422b092659",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e48e23-f4ca-4e3f-a988-9e6b30413996",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea6a0d-451c-4eae-bd04-206b7b44bcc8",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadafc21-6079-4c6b-b35e-2c43a6b93b17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "folder_path = \"corpus\"\n",
    "folder_path\n",
    "\n",
    "# Function to calculate text metrics\n",
    "def calculate_metrics(text):\n",
    "    sentences = re.split(r'[.!?]', text)  # Split text into sentences\n",
    "    sentence_count = len([s for s in sentences if s.strip()])  # Count non-empty sentences\n",
    "    words = text.split()  # Split text into words\n",
    "    word_count = len(words)  # Total word count\n",
    "    average_sentence_length = word_count / sentence_count if sentence_count > 0 else 0  # Average sentence length\n",
    "    \n",
    "    return word_count, sentence_count, average_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcc529-b913-4f02-a9d2-cf1681aeefa1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists \n",
    "folder_path = '../corpus'\n",
    "data = []\n",
    "\n",
    "all_word_counts = []\n",
    "all_sentence_counts = []\n",
    "all_avg_sentence_lengths = []\n",
    "\n",
    "# Loop through the two text files in the folder. \n",
    "# Note: \"all_gregory\" consists of all of the Lady Gregory texts used and \"all_original\" consists of the translations of the original legends.\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename in ['all_gregory.txt', 'all_original.txt']: \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Tokenize text into sentences and words\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            words = re.findall(r'\\w+', text)\n",
    "            \n",
    "            # Quantitative analysis\n",
    "            sentence_count = len(sentences)\n",
    "            word_count = len(words)\n",
    "            avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
    "            \n",
    "            # Assign author category based on the file name\n",
    "            if filename == 'all_gregory.txt':\n",
    "                author = 'Gregory'\n",
    "            elif filename == 'all_original.txt':\n",
    "                author = 'Original'\n",
    "            \n",
    "            # Append results to data list\n",
    "            all_word_counts.append(word_count)\n",
    "            all_sentence_counts.append(sentence_count)\n",
    "            all_avg_sentence_lengths.append(avg_words_per_sentence)\n",
    "            \n",
    "            data.append({\n",
    "                'File Name': filename,\n",
    "                'Author': author,\n",
    "                'Word Count': word_count,\n",
    "                'Sentence Count': sentence_count,\n",
    "                'Sentence Length': avg_words_per_sentence\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec04ede-3dcb-45d1-a6a1-f5ef2c288275",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7695d-6071-4d95-95a0-9dd72f87867f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculate the average of each column \n",
    "avg_word_count = df['Word Count'].mean()\n",
    "avg_sentence_count = df['Sentence Count'].mean()\n",
    "avg_sentence_length = df['Sentence Length'].mean()\n",
    "\n",
    "# calculate the average of each column by author\n",
    "df_by_author = df.groupby('Author').mean(numeric_only=True)\n",
    "df_by_author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cebb7c-fc4a-4acd-a0c7-92f7e4d45dc6",
   "metadata": {},
   "source": [
    "## Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47b387-b6f9-4ce1-8ab5-0b88ce71fdc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### - General Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b532b-fbc3-497d-b4f7-613701ceeefc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove [p. X] pattern (specific to some texts)\n",
    "    text = re.sub(r\"\\[p\\. \\d+\\]\", \"\", text)\n",
    "    \n",
    "    # Remove specific text such as book title and source information, if present\n",
    "    text = re.sub(r\"Cuchulain of Muirthemne,? by Lady Augusta Gregory,? \\[1902\\], \"\", \n",
    "                  text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"paragraph continues\", \"\", text)\n",
    "\n",
    "    # Remove special characters and numbers except spaces\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    html_regex = re.compile('<.*?>')\n",
    "    text = html_regex.sub('', text)\n",
    "    \n",
    "    # Replace newline characters with spaces\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9fc43-35dc-4582-b693-968ba154a390",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corpus_folder=\"../corpus\"\n",
    "tokenized_g = []\n",
    "tokenized_o = []\n",
    "\n",
    "# File names \n",
    "g_file = 'all_gregory.txt'\n",
    "o_file = 'all_original.txt'\n",
    "\n",
    "# Tokenize gregory file\n",
    "g_file_path = os.path.join(corpus_folder, g_file)\n",
    "with open(g_file_path, 'r', encoding='utf-8') as file:\n",
    "    g_text = file.read()\n",
    "    cleaned_g_text = clean_text(g_text)  # Clean the text\n",
    "    tokenized_g = word_tokenize(cleaned_g_text)  # Tokenize the cleaned text\n",
    "\n",
    "# Tokenize original file\n",
    "o_file_path = os.path.join(corpus_folder, o_file)\n",
    "with open(o_file_path, 'r', encoding='utf-8') as file:\n",
    "    o_text = file.read()\n",
    "    cleaned_o_text = clean_text(o_text)  # Clean the text\n",
    "    tokenized_o = word_tokenize(cleaned_o_text)  # Tokenize the cleaned text\n",
    "\n",
    "# Paths to store tokenized files\n",
    "tokenized_g_file_path = os.path.join(corpus_folder, 'tokenized_g.txt')\n",
    "tokenized_o_file_path = os.path.join(corpus_folder, 'tokenized_o.txt')\n",
    "\n",
    "# Save tokenized g text to a file\n",
    "with open(tokenized_g_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(' '.join(tokenized_g))  # Join tokens with spaces\n",
    "\n",
    "# Save tokenized o text to a file\n",
    "with open(tokenized_o_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(' '.join(tokenized_o))  # Join tokens with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc4ac0-ac52-4e8a-abee-8a886d54b6c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove all single letter words except for \"a\", \"i\", and \"o\"\n",
    "# these are the only three valid one-letter words in the English language\n",
    "g_letter = [word for word in tokenized_g if len(word) != 1 or word.lower() in ('a', 'i', 'o')]\n",
    "o_letter = [word for word in tokenized_o if len(word) != 1 or word.lower() in ('a', 'i', 'o')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e8a82-86ca-4d59-824c-ac8443a009de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove function words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "g_text = [word for word in g_letter if word.lower() not in stop_words]\n",
    "o_text = [word for word in o_letter if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cd09c-bc10-4f05-aa56-3186db9a2d6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove archaic words found in original texts (\"thy\", \"thee\", etc.), character names, and place names except for \"Ireland\" and it's variants \n",
    "all_names = ['Táin bo Cuailgne', 'crachain ', \"eochaid\", 'ferceirtne', 'etercels', 'domnand', 'ferbeson', 'craiftine', 'beara', 'aife', 'conaing', 'conlaoch', 'culain', 'crandce', 'deride', 'feochar', 'cerdda', 'chula', 'comainm', 'feoraind', 'culands', 'duma', 'cormac', 'connad', 'eterscele', 'forgaimin', 'conrach', 'cerna', 'ferb', 'eruaine', 'bunne', 'ferchon', 'donegal', 'deroil', 'fer', 'forais', 'aife', 'chulainn', 'ferdach', 'fn', 'culann', 'fun', 'feradach', 'foras', 'aife', 'clath', 'chulainmany', 'culend', 'dundalk', 'fngin', 'banba', 'bithln', 'conroi', 'coscra', 'ferde', 'ethal', 'foraoi', 'ferbend', 'comchraid', 'connall', 'cru', 'dunfor', 'ad', 'agnomon', 'brecc', 'fain', 'banba', 'aife', 'ailell', 'ailells', 'ailill', 'ainnle', 'alban', 'amergin', 'anbual', 'angus', 'aoife', 'ardan', 'art', 'assaroe', 'atha', 'athach', 'athairne', 'athanforaire', 'athcoltna', 'athgowla', 'athisech', 'bachlach', 'banba', 'banbach', 'banchuig', 'banchuing', 'bel', 'bithln', 'blai', 'blaicne', 'blaitmic', 'blathnat', 'boann', 'bodb', 'boinne', 'brecc', 'breg', 'bregia', 'bregians', 'bregoes', 'bregons', 'bregros', 'bregs', 'bren', 'brenguir', 'breoga', 'breogann', 'bres', 'bresal', 'bresel', 'breslech', 'breslige', 'brian', 'bricriu', 'brod', 'buadach', 'byres', 'caer', 'caerthund', 'cailnge', 'caladcholg', 'calatin', 'calatins', 'canchuig', 'cathbad', 'cearnach', 'cecht', 'celthair', 'celthairs', 'cernach', 'chetaig', 'chetail', 'chilair', 'chille', 'chliss', 'chon', 'chrachan', 'chrachna', 'chulain', 'chulainn', 'chulainns', 'chulainnwise', 'chulalnns', 'ciallglind', 'cinn', 'clath', 'clothach', 'cona', 'conachar', 'conachars', 'conail', 'conaille', 'conaire', 'conall', 'conchobar', 'conchobars', 'conchobor', 'conchubar', 'conchubars', 'condere', 'conlaoch', 'conloingeas', 'connacht', 'connaght', 'connaid', 'connall', 'connaught', 'connaughts', 'connla', 'connud', 'conor', 'conors', 'conrach', 'corco', 'cork', 'cormac', \"cormac's\", 'cormacs', 'cornkiln', 'couch', 'cr', 'crachain', 'crachna', 'crachu', 'craiftine', 'crandce', 'crannach', 'crantail', 'crantails', 'crfoit', 'crft', 'criadh', 'crichid', 'crn', 'cro', 'cruachan', 'cruachs', 'cruahan', 'cruaidin', 'cruife', 'cruifne', 'cscraid', 'cu', 'cuailgne', 'cualgne', 'cuchulain', 'cuchulaind', 'cuchulainn', 'cuchulains', 'cuchullin', 'cuillenn', 'culain', 'culann', 'curad', 'curoi', 'dagda', 'daire', 'dalach', 'damach', 'daman', 'damin', \"said\", 'danaan', 'darius', 'dechtire', 'deidre', 'deirdre', 'delgan', 'dessa', 'diad', 'diman', 'donall', 'donnell', 'dost', 'draccon', 'drimne', 'dris', 'driscoll', 'drucht', 'dundealgan', 'durthacht', 'ed', 'edond', 'egem', 'ehmain', 'eidre', 'eirr', 'eirrgi', 'eisi', 'eit', 'eitche', 'eithlend', ':', 'Edinburgh MS', 'eithlin', 'eithne', 'ele', 'elga', 'ellann', 'ellonn', 'emain', 'eman', 'emania', 'emer', 'emers', 'emna', 'enchenn', 'eogan', 'eoghan', 'eterscel', 'ethal', 'ethgowla', 'ethne', 'fachaig', 'fachtna', 'fachtnas', 'faindle', 'fainnle', 'fand', 'fedelmid', 'fedlimid', 'fer', 'ferb', 'ferdiad', 'ferg', 'fergel', 'ferger', 'fergne', 'fergus', 'ferloga', 'ferobain', 'ferogain', 'fiannamail', 'filliud', 'findabair', 'findabairs', 'findach', 'findairget', 'findbennach', 'findchad', 'findchoem', 'findchoimi', 'findderg', 'findomain', 'findruine', 'fingalach', 'fingan', 'fingin', 'finishthe', 'finn', 'finnabair', 'finnabar', 'finncairn', 'finnchairn', 'finnched', 'finnchoem', 'fintans', 'firb', 'firecearna', 'firgein', 'fngin', 'fomor', 'forbuide', 'forcul', 'fordofthemrrigan', 'fordui', 'forgaimin', 'forgall', 'fornuil', 'foscul', 'fosse', 'fota', 'fraech', 'fraidtine', 'frechlethain', 'frecul', 'fretted', 'fri', 'friuch', 'furachar', 'furbaidewhite', 'furbuithe', 'geanann', 'genann', 'gerg', 'gergs', 'hast', 'hath', 'ibar', 'ie', 'ilsuanach', 'ingcel', 'ini', 'iollan', 'iubar', 'labra', 'labraid', 'laeg', 'laegaire', 'laegaires', 'laegh', 'lecca', 'lenister', 'lethan', 'levarcham', 'liban', 'loigaire', 'lomna', 'lugai', 'lugaid', 'mac', 'macha', 'mache', 'maev', 'maeve', 'maeves', 'magh', 'maine', 'manannan', 'mane', 'mani', 'meave', 'medb', 'meix', 'muirthemne', 'munster', 'mve', 'naisi', 'naoise', 'nechtain', 'nechtan', 'nera', 'ness', 'niafer', 'niall', 'niamh', 'oengus', 'quoth', 'rogain', 'rogh', 'roig', 'scathach', 'setanta', 'shalt', 'sid', 'slebe', 'sliab fuait', 'slieve', 'sualtaim', 'sualtim', 'tata', 'teamhair', 'thee', 'thine', 'thou', 'thus', 'thy', 'tis', 'ulster', 'usnach', 'usnas', 'viz', 'wilt', 'ye']\n",
    "\n",
    "# remove duplicates\n",
    "names = set(all_names)\n",
    "print(f\"Unique names: {len(names)} total\")\n",
    "\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(name) for name in names) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Remove names\n",
    "g_final_text = [word for word in g_text if word.lower() not in names]\n",
    "o_final_text = [word for word in o_text if word.lower() not in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8b932-d564-46a7-a5c9-c0b0c77ca519",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "g_count = Counter(g_final_text)\n",
    "o_count = Counter(o_final_text)\n",
    "\n",
    "top_10_g = g_count.most_common(10)\n",
    "top_10_o = o_count.most_common(10)\n",
    "\n",
    "print(\"Most Common Gregory Words:\")\n",
    "print(top_10_g)\n",
    "print(\"\\n\")\n",
    "print(\"Most Common Original Words:\")\n",
    "print(top_10_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048aedb4-ec58-4b3a-abd1-9501937cb55b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "unique_to_g = {word for word, count in g_count.items() if count > o_count[word] and o_count[word] < 5}\n",
    "print(\"Most Common Words Unique to Gregory:\")\n",
    "print(sorted(unique_to_g, key=lambda word: g_count[word], reverse=True)[0:20])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "unique_to_o = {word for word, count in o_count.items() if count > g_count[word] and g_count[word] < 5}\n",
    "#unique_to_o = {word for word, count in o_count.items() if count > g_count[word]}\n",
    "print(\"Most Common Words Unique to Original:\")\n",
    "print(sorted(unique_to_o, key=lambda word: o_count[word], reverse=True)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b49022-d354-4dc2-a48a-8228e86b2116",
   "metadata": {},
   "source": [
    "#### - Frequency of specific keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17067d-0a0d-4e2f-a238-7a5dc85da581",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this is to check if the words in the \"words_to_check\" list actually do appear in the texts\n",
    "\n",
    "words_to_check = [\"sidhe\", \"fairy\", \"magic\"]\n",
    "\n",
    "# Get counts from g_count and o_count\n",
    "g_counts = {word: g_count[word] for word in words_to_check}\n",
    "o_counts = {word: o_count[word] for word in words_to_check}\n",
    "\n",
    "print(\"Counts in Gregory's Texts:\")\n",
    "for word, count in g_counts.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nCounts in Original Texts:\")\n",
    "for word, count in o_counts.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a9ec1-a02a-4f4c-aaec-8feb577aa890",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce8be0-f8dd-46a5-813a-bd99c186c328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# normalized frequencies\n",
    "male_counts = ['man', 'men', 'boy', 'boyhood', 'lad', 'husband', 'father', 'son', 'male', 'brother', 'grandfather', 'uncle', 'nephew', 'hero', 'gentleman', 'guy', 'king', 'prince', 'fiance', 'fiancé', 'patriarch', 'patriarch', 'master']\n",
    "female_counts = ['woman', 'women', 'wife', 'girl', 'female', 'daughter', 'lady', 'mother', 'sister', 'fiancee', 'fiancée', 'aunt', 'niece', 'girlfriend', 'queen', 'womanhood', 'princess', 'matriarch', 'damsel', 'mistress', 'heroine', 'matron', 'goddess', 'hag', 'feminine']\n",
    "\n",
    "# Calculate totals for Gregory's counts\n",
    "total_male_g = sum(g_count[word] for word in male_counts if word in g_count)\n",
    "total_female_g = sum(g_count[word] for word in female_counts if word in g_count)\n",
    "\n",
    "# Calculate totals for Original counts\n",
    "total_male_o = sum(o_count[word] for word in male_counts if word in o_count)\n",
    "total_female_o = sum(o_count[word] for word in female_counts if word in o_count)\n",
    "\n",
    "# Total word counts in each corpus\n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566 \n",
    "\n",
    "# Normalize the counts (by dividing by total word counts)\n",
    "normalized_male_g = total_male_g / total_words_gregory\n",
    "normalized_female_g = total_female_g / total_words_gregory\n",
    "\n",
    "normalized_male_o = total_male_o / total_words_original\n",
    "normalized_female_o = total_female_o / total_words_original\n",
    "\n",
    "# Print the normalized results\n",
    "print(f\"Normalized Male Counts in Gregory's Texts: {normalized_male_g:.6f}\")\n",
    "print(f\"Normalized Female Counts in Gregory's Texts: {normalized_female_g:.6f}\")\n",
    "print(f\"Normalized Male Counts in Original Texts: {normalized_male_o:.6f}\")\n",
    "print(f\"Normalized Female Counts in Original Texts: {normalized_female_o:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c545ec1-4c0a-402b-b5e5-db6a2e231040",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# retrieved from counting the occurrence of each keyword\n",
    "\n",
    "# Counts in Gregory's Texts\n",
    "gregory_counts = [('man', 352), ('men', 706), ('boy', 54), ('lad', 22), ('husband', 26), ('father', 48), ('son', 335), ('brother', 13),  ('hero', 30), ('king', 274), ('prince', 4), ('master', 25), ('woman', 120), ('women', 144), ('wife', 74), ('girl', 34), ('daughter', 68), ('lady', 0), ('mother', 33), ('sister', 11), ('queen', 33)]\n",
    "\n",
    "# Counts in Original Texts\n",
    "original_counts = [('man', 338), ('men', 564), ('boy', 215), ('lad', 71), ('husband', 14), ('father', 46), ('son', 353), ('brother', 8),  ('hero', 62), ('king', 258), ('prince', 15), ('master', 46), ('woman', 108), ('women', 108), ('wife', 75), ('girl', 18), ('daughter', 100), ('lady', 31), ('mother', 38), ('sister', 11), ('queen', 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caa6c9-4b67-43c7-b617-7ff8489db828",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# retrieved from counting the occurrence of each keyword\n",
    "\n",
    "# Counts in Gregory's Texts\n",
    "gregory_counts = [('man', 352), ('men', 706), ('boy', 54), ('husband', 26), ('father', 48), ('son', 335), ('brother', 13), ('king', 274), \n",
    "                  ('woman', 120), ('women', 144), ('girl', 34), ('wife', 74), ('mother', 33), ('daughter', 68), ('sister', 11), ('queen', 33)]\n",
    "\n",
    "# Counts in Original Texts\n",
    "original_counts = [('man', 338), ('men', 564), ('boy', 215), ('husband', 14), ('father', 46), ('son', 353), ('brother', 8), ('king', 258),  \n",
    "                   ('woman', 108), ('women', 108), ('girl', 18), ('wife', 75), ('mother', 38), ('daughter', 100), ('sister', 11), ('queen', 35)]\n",
    "\n",
    "# Total word counts in each text \n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566  \n",
    "\n",
    "# Prepare data for plotting \n",
    "words_to_check = [word for word, _ in gregory_counts]\n",
    "\n",
    "# Calculate normalized frequencies\n",
    "g_values = [count / total_words_gregory for _, count in gregory_counts]\n",
    "o_values = [count / total_words_original for _, count in original_counts]\n",
    "\n",
    "# Bar chart parameters\n",
    "x = np.arange(len(words_to_check))  # Word indices\n",
    "bar_width = 0.35\n",
    "\n",
    "# Create the grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "bars1 = ax.bar(x - bar_width / 2, g_values, bar_width, label=\"Gregory's Texts\", color='limegreen')\n",
    "bars2 = ax.bar(x + bar_width / 2, o_values, bar_width, label=\"Original Texts\", color='dodgerblue')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_ylabel('Normalized Frequency')\n",
    "ax.set_title('Normalized Word Counts in Gregory\\'s and Original Texts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(words_to_check, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0c8a0-29d1-4857-8929-09fae2bb50fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Nationalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633f2ea-c115-4bb9-9be5-ac695a92a79e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_counts = Counter(g_final_text)\n",
    "o_counts = Counter(o_final_text)\n",
    "nationalism_count = ['nation', 'nationalism', 'partition', 'resistance', 'struggle', 'heritage', 'oppression', 'freedom', 'power', 'rule', 'control', 'king', 'queen', 'leader', 'government', 'throne', 'monarch', 'reign', 'influence', 'command', 'royal', 'law', 'command']\n",
    "\n",
    "# Calculate totals for Gregory's counts\n",
    "total_n_g = sum(g_counts[word] for word in nationalism_count if word in g_counts)\n",
    "total_n_o = sum(o_counts[word] for word in nationalism_count if word in o_counts)\n",
    "\n",
    "# Total word counts in each corpus\n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566 \n",
    "\n",
    "# Normalize the counts (by dividing by total word counts)\n",
    "normalized_n_g = total_n_g / total_words_gregory\n",
    "normalized_n_o = total_n_o / total_words_original\n",
    "\n",
    "# Print the normalized results\n",
    "print(f\"Normalized Nationalism Counts in Gregory's Texts: {normalized_n_g:.6f}\")\n",
    "print(f\"Normalized Nationalism Counts in Original Texts: {normalized_n_o:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4bd3d-7d1c-4406-af6c-5b45df2fb0cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_counts = Counter(g_final_text)\n",
    "o_counts = Counter(o_final_text)\n",
    "nationalism_count = ['ulster', 'ulstermen', 'ulsterman', 'ulstermans', 'ulstermens', 'ulonia', 'ultonian', 'ultonians']\n",
    "                     \n",
    "# Calculate totals for Gregory's counts\n",
    "total_n_g = sum(g_counts[word] for word in nationalism_count if word in g_counts)\n",
    "total_n_o = sum(o_counts[word] for word in nationalism_count if word in o_counts)\n",
    "\n",
    "# Total word counts in each corpus\n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566 \n",
    "\n",
    "# Normalize the counts (by dividing by total word counts)\n",
    "normalized_n_g = total_n_g / total_words_gregory\n",
    "normalized_n_o = total_n_o / total_words_original\n",
    "\n",
    "# Print the normalized results\n",
    "print(f\"Normalized Ulster Counts in Gregory's Texts: {normalized_n_g:.6f}\")\n",
    "print(f\"Normalized Ulster Counts in Original Texts: {normalized_n_o:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610dc118-db99-491b-acff-bbfac1e505e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_counts = Counter(g_final_text)\n",
    "o_counts = Counter(o_final_text)\n",
    "nationalism_count = [\"ireland\", 'irish', 'irishman', \"irishmen\", \"irishwomen\", \"irishwoman\", \"celt\", \"celtic\", \"celts\", \"hibernia\", \"eire\", \"eireann\", \"éire\", \"island\", \"inis fail\"]\n",
    "\n",
    "# Calculate totals for Gregory's counts\n",
    "total_n_g = sum(g_counts[word] for word in nationalism_count if word in g_counts)\n",
    "total_n_o = sum(o_counts[word] for word in nationalism_count if word in o_counts)\n",
    "\n",
    "# Total word counts in each corpus\n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566 \n",
    "\n",
    "# Normalize the counts (by dividing by total word counts)\n",
    "normalized_n_g = total_n_g / total_words_gregory\n",
    "normalized_n_o = total_n_o / total_words_original\n",
    "\n",
    "# Print the normalized results\n",
    "print(f\"Normalized Counts in Gregory's Texts: {normalized_n_g:.6f}\")\n",
    "print(f\"Normalized Counts in Original Texts: {normalized_n_o:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857d1a0-2b83-48a6-8e04-5aabb2d62706",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Folklore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1654aac-73da-4d0c-935d-8c4fa0ff65dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_counts = Counter(g_final_text)\n",
    "o_counts = Counter(o_final_text)\n",
    "folklore_count = [\"sidhe\", \"fairy\", \"magic\", \"mystic\", \"pastoral\", \"tranquil\", \"ethereal\", \"timeless\", \"mystical\", \"enchanting\", \"golden age\", \"mythic\", \"otherworldly\", \"sacred\", \"humble\", \"simple\", \"harmony\", \"rustic\", \"idyllic\", \"idealized\", \"ideal\", \"picturesque\", \"romantic\", \"unspoiled\", 'folklore', 'spirit', 'supernatural', 'sorcery', 'witch', 'wizard', 'specter', 'dream', 'enchantment', 'spell', 'monster', 'myth', 'prophecy', 'legend', 'curse', 'spectre', 'ghost', 'occult', 'vision', 'charm']\n",
    "\n",
    "# Calculate totals for Gregory's counts\n",
    "total_f_g = sum(g_counts[word] for word in folklore_count if word in g_counts)\n",
    "total_f_o = sum(o_counts[word] for word in folklore_count if word in o_counts)\n",
    "\n",
    "# Total word counts in each corpus\n",
    "total_words_gregory = 123522  \n",
    "total_words_original = 147566 \n",
    "\n",
    "# Normalize the counts (by dividing by total word counts)\n",
    "normalized_f_g = total_f_g / total_words_gregory\n",
    "normalized_f_o = total_f_o / total_words_original\n",
    "\n",
    "# Print the normalized results\n",
    "print(f\"Normalized Folklore Counts in Gregory's Texts: {normalized_f_g:.6f}\")\n",
    "print(f\"Normalized Folklore Counts in Original Texts: {normalized_f_o:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d72bb-83c5-4816-a804-7d50794e22c2",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0731954-3676-4eb9-aa04-15e08a468ab5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# turn strings into tokens\n",
    "g_topic = [word_tokenize(doc) for doc in g_final_text]\n",
    "o_topic = [word_tokenize(doc) for doc in o_final_text]\n",
    "\n",
    "print(g_final_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c17c0-c7ea-477e-9ca7-fa76d5afba77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create dictionaries for each dataset\n",
    "dictionary_g = Dictionary(g_topic)\n",
    "dictionary_o = Dictionary(o_topic)\n",
    "\n",
    "# Create corpora for each dataset\n",
    "corpus_g = [dictionary_g.doc2bow(doc) for doc in g_topic]\n",
    "corpus_o = [dictionary_o.doc2bow(doc) for doc in o_topic]\n",
    "\n",
    "print(corpus_g[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48034b-8729-4ebf-a78e-23dc6bce5350",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Train LDA for each dataset\n",
    "lda_g = LdaModel(corpus=corpus_g, id2word=dictionary_g, num_topics=num_topics, passes=10)\n",
    "lda_o = LdaModel(corpus=corpus_o, id2word=dictionary_o, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Print the topics for both datasets\n",
    "print(\"Topics in tokenized_g:\")\n",
    "for idx, topic in lda_g.print_topics(num_topics=num_topics, num_words=6):\n",
    "    print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "print(\"\\nTopics in tokenized_o:\")\n",
    "for idx, topic in lda_o.print_topics(num_topics=num_topics, num_words=6):\n",
    "    print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "# for reference:\n",
    "# g order of prevalence:  1, 3, 5, 4, 2 (Ireland, persistence, grief, life/death, family)\n",
    "# o order of prevalence: 3, 4, 2, 5, 1 (royalty, creativity, journeys, performance, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86efe3-85e8-4248-894d-1ffc3190e813",
   "metadata": {},
   "source": [
    "**Gregory** *- focuses on emotions, Ireland, family, love, and grief*\n",
    "- Topic 1: movement and **Ireland**\n",
    "- Topic 2: **family** and death\n",
    "- Topic 3: effort and **persistence**\n",
    "- Topic 4: life and death\n",
    "- Topic 5: grief, loss, and journey\n",
    "\n",
    "**Original** *- focuses on creativity, royalty, mythology, and adventure*\n",
    "- Topic 1: numbers and groups\n",
    "- Topic 2: journeys and fairness\n",
    "- Topic 3: authority and **royalty**\n",
    "- Topic 4: family and creativity\n",
    "- Topic 5: youth, performance, and renown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268426c6-17fd-4ae5-8dcf-0dd1210fc9c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the topic proportions for each document in the corpus\n",
    "topic_proportions = [lda_g.get_document_topics(doc) for doc in corpus_g]\n",
    "\n",
    "# Calculate the average proportion of each topic across all documents\n",
    "average_topic_proportions = []\n",
    "for topic in range(lda_g.num_topics):\n",
    "    # Sum the proportions for the current topic across all documents\n",
    "    topic_sum = sum([dict(doc).get(topic, 0) for doc in topic_proportions])\n",
    "    average_topic_proportions.append(topic_sum / len(corpus_g))  \n",
    "\n",
    "\n",
    "# Calculate the topic proportions for each document in corpus_o\n",
    "topic_proportions_o = [lda_o.get_document_topics(doc) for doc in corpus_o]\n",
    "\n",
    "# Calculate the average proportion of each topic across all documents in corpus_o\n",
    "average_topic_proportions_o = []\n",
    "for topic in range(lda_o.num_topics):\n",
    "    # Sum the proportions for the current topic across all documents in corpus_o\n",
    "    topic_sum_o = sum([dict(doc).get(topic, 0) for doc in topic_proportions_o])\n",
    "    average_topic_proportions_o.append(topic_sum_o / len(corpus_o))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008255b-157a-4b46-a6f3-4ca7107310f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(average_topic_proportions)\n",
    "# 20.3% of g is associated with topic 0, 19.7% with 1, 20% with 2, 19.8% with 3, 20% with 4\n",
    "# order of prevalence: 1, 3, 5, 4, 2\n",
    "\n",
    "print(average_topic_proportions_o)\n",
    "#19.8% of o is associated with topic 0, 19.9% with 1, 20.7% with 2, 19.9% with 3, 19.8% with 4\n",
    "# order of prevalence: 3, 4, 2, 5, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1ae7b-7dbf-4028-aabe-67065e087626",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Topic labels for corpus_g and corpus_o\n",
    "labels_g = ['Ireland', 'Family', 'Persistence', 'Life and Death', 'Emotions']\n",
    "labels_o = ['Groups', 'Journeys', 'Royalty', 'Creativity', 'Performance']\n",
    "\n",
    "# Average topic proportions for each corpus (from previous result)\n",
    "average_topic_proportions = [0.2032401225306466, 0.19736801222659592, 0.2006235324836632, 0.19828410820549436, 0.2004842252319235]\n",
    "average_topic_proportions_o = [0.1976806726128636, 0.19854617684159165, 0.20656051957863697, 0.1990046764742016, 0.19820795491123766]\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))  \n",
    "\n",
    "# Plot for corpus_g\n",
    "ax1.bar(range(5), average_topic_proportions, width=0.5, color='limegreen')\n",
    "ax1.set_xlabel('Primary Theme of each Topic')\n",
    "ax1.set_ylabel('Average Prevalence (Proportion)')\n",
    "ax1.set_title(\"Topic Prevalence in Gregory's texts\")\n",
    "ax1.set_xticks(range(5))\n",
    "ax1.set_xticklabels(labels_g)\n",
    "ax1.set_ylim(0.195, 0.208)\n",
    "ax1.grid(True, which='both', axis='y', color='lightgrey', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot for corpus_o\n",
    "ax2.bar(range(5), average_topic_proportions_o, width=0.5, color=\"dodgerblue\")\n",
    "ax2.set_xlabel('Primary Theme of each Topic')\n",
    "ax2.set_ylabel('Average Prevalence (Proportion)')\n",
    "ax2.set_title(\"Topic Prevalence in Original Texts\")\n",
    "ax2.set_xticks(range(5))\n",
    "ax2.set_xticklabels(labels_o)\n",
    "ax2.set_ylim(0.195, 0.208)\n",
    "ax2.grid(True, which='both', axis='y', color='lightgrey', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8c609-2b67-4021-86a4-2a4d3efd46a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# interactive graph\n",
    "pyLDAvis.display(lda_vis_g)  \n",
    "pyLDAvis.display(lda_vis_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c092a97-e4fd-4919-ad12-74d9d6eb085d",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282ddb0-68b8-42d9-a9b9-5f3eaaf685ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove archaic words found in original texts (\"thy\", \"thee\", etc.), character names, and place names except for \"Ireland\" and its variants \n",
    "all_names = ['Táin bo Cuailgne', 'crachain ', 'ferceirtne', 'etercels', 'domnand', 'ferbeson', 'craiftine', 'beara', 'aife', 'conaing', 'conlaoch', 'culain', 'crandce', 'deride', 'feochar', 'cerdda', 'chula', 'comainm', 'feoraind', 'culands', 'duma', 'cormac', 'connad', 'eterscele', 'forgaimin', 'conrach', 'cerna', 'ferb', 'eruaine', 'bunne', 'ferchon', 'donegal', 'deroil', 'fer', 'forais', 'aife', 'chulainn', 'ferdach', 'fn', 'culann', 'fun', 'feradach', 'foras', 'aife', 'clath', 'chulainmany', 'culend', 'dundalk', 'fngin', 'banba', 'bithln', 'conroi', 'coscra', 'ferde', 'ethal', 'foraoi', 'ferbend', 'comchraid', 'connall', 'cru', 'dunfor', 'ad', 'agnomon', 'brecc', 'fain', 'banba', 'aife', 'ailell', 'ailells', 'ailill', 'ainnle', 'alban', 'amergin', 'anbual', 'angus', 'aoife', 'ardan', 'art', 'assaroe', 'atha', 'athach', 'athairne', 'athanforaire', 'athcoltna', 'athgowla', 'athisech', 'bachlach', 'banba', 'banbach', 'banchuig', 'banchuing', 'bel', 'bithln', 'blai', 'blaicne', 'blaitmic', 'blathnat', 'boann', 'bodb', 'boinne', 'brecc', 'breg', 'bregia', 'bregians', 'said', 'bregoes', 'bregons', 'bregros', 'bregs', 'bren', 'brenguir', 'breoga', 'breogann', 'bres', 'bresal', 'bresel', 'breslech', 'breslige', 'brian', 'bricriu', 'brod', 'buadach', 'byres', 'caer', 'caerthund', 'cailnge', 'caladcholg', 'calatin', 'calatins', 'canchuig', 'cathbad', 'cearnach', 'cecht', 'celthair', 'celthairs', 'cernach', 'chetaig', 'chetail', 'chilair', 'chille', 'chliss', 'chon', 'chrachan', 'chrachna', 'chulain', 'chulainn', 'chulainns', 'chulainnwise', 'chulalnns', 'ciallglind', 'cinn', 'clath', 'clothach', 'cona', 'conachar', 'conachars', 'conail', 'conaille', 'conaire', 'conall', 'conchobar', 'conchobars', 'conchobor', 'conchubar', 'conchubars', 'condere', 'conlaoch', 'conloingeas', 'connacht', 'connaght', 'connaid', 'connall', 'connaught', 'connaughts', 'connla', 'connud', 'conor', 'conors', 'conrach', 'corco', 'cork', 'cormac', \"cormac's\", 'cormacs', 'cornkiln', 'couch', 'cr', 'crachain', 'crachna', 'crachu', 'craiftine', 'crandce', 'crannach', 'crantail', 'crantails', 'crfoit', 'crft', 'criadh', 'crichid', 'crn', 'cro', 'cruachan', 'cruachs', 'cruahan', 'cruaidin', 'cruife', 'cruifne', 'cscraid', 'cu', 'cuailgne', 'cualgne', 'cuchulain', 'cuchulaind', 'cuchulainn', 'cuchulains', 'cuchullin', 'cuillenn', 'culain', 'culann', 'curad', 'curoi', 'dagda', 'daire', 'dalach', 'damach', 'daman', 'damin', 'danaan', 'darius', 'dechtire', 'deidre', 'deirdre', 'delgan', 'dessa', 'diad', 'diman', 'donall', 'donnell', 'dost', 'draccon', 'drimne', 'dris', 'driscoll', 'drucht', 'dundealgan', 'durthacht', 'ed', 'edond', 'egem', 'ehmain', 'eidre', 'eirr', 'eirrgi', 'eisi', 'eit', 'eitche', 'eithlend', ':', 'Edinburgh MS', 'eithlin', 'eithne', 'ele', 'elga', 'ellann', 'ellonn', 'emain', 'eman', 'emania', 'emer', 'emers', 'emna', 'enchenn', 'eogan', 'eoghan', 'eterscel', 'ethal', 'ethgowla', 'ethne', 'fachaig', 'fachtna', 'fachtnas', 'faindle', 'fainnle', 'fand', 'fedelmid', 'fedlimid', 'fer', 'ferb', 'ferdiad', 'ferg', 'fergel', 'ferger', 'fergne', 'fergus', 'ferloga', 'ferobain', 'ferogain', 'fiannamail', 'filliud', 'findabair', 'findabairs', 'findach', 'findairget', 'findbennach', 'findchad', 'findchoem', 'findchoimi', 'findderg', 'findomain', 'findruine', 'fingalach', 'fingan', 'fingin', 'finishthe', 'finn', 'finnabair', 'finnabar', 'finncairn', 'finnchairn', 'finnched', 'finnchoem', 'fintans', 'firb', 'firecearna', 'firgein', 'fngin', 'fomor', 'forbuide', 'forcul', 'fordofthemrrigan', 'fordui', 'forgaimin', 'forgall', 'fornuil', 'foscul', 'fosse', 'fota', 'fraech', 'fraidtine', 'frechlethain', 'frecul', 'fretted', 'fri', 'friuch', 'furachar', 'furbaidewhite', 'furbuithe', 'geanann', 'genann', 'gerg', 'gergs', 'hast', 'hath', 'ibar', 'ie', 'ilsuanach', 'ingcel', 'ini', 'iollan', 'iubar', 'labra', 'labraid', 'laeg', 'laegaire', 'laegaires', 'laegh', 'lecca', 'lenister', 'lethan', 'levarcham', 'liban', 'loigaire', 'lomna', 'lugai', 'lugaid', 'mac', 'macha', 'mache', 'maev', 'maeve', 'maeves', 'magh', 'maine', 'manannan', 'mane', 'mani', 'meave', 'medb', 'meix', 'muirthemne', 'munster', 'mve', 'naisi', 'naoise', 'nechtain', 'nechtan', 'nera', 'ness', 'niafer', 'niall', 'niamh', 'oengus', 'quoth', 'rogain', 'rogh', 'roig', 'scathach', 'setanta', 'shalt', 'sid', 'slebe', 'sliab fuait', 'slieve', 'sualtaim', 'sualtim', 'tata', 'teamhair', 'thee', 'thine', 'thou', 'thus', 'thy', 'tis', 'ulster', 'usnach', 'usnas', 'viz', 'wilt', 'ye']\n",
    "\n",
    "# remove duplicates\n",
    "names = set(all_names)\n",
    "print(f\"Unique names: {len(names)} total\")\n",
    "\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(name) for name in names) + r')\\b', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9be104-9e8d-42fc-92eb-3d64a1ce9895",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"../corpus/labelled\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "corpus = []\n",
    "for file in files:\n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        # Replace matched names with an empty string\n",
    "        cleaned_text = pattern.sub(\"\", text)\n",
    "        corpus.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422ad49-c0c0-4507-88da-53fdd1060fdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create a TF-IDF matrix pca\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053eed83-9459-430b-8477-91418ff2dd9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# apply PCA to TF-IDF matrix\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b31a7-f179-40e0-b4fa-256fdc65a08d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# examine loadings of the first principal component\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "comps = pca.components_.transpose()\n",
    "vocab_weights = sorted(zip(comps[:, 0], vocab))\n",
    "\n",
    "print('Positive loadings:')\n",
    "print('\\t \\t'.join(f'{w} -> {s:.3f}' for s, w in vocab_weights[:25]))\n",
    "print(\" \")\n",
    "print('Negative loadings:')\n",
    "print('\\t \\t'.join(f'{w} -> {s:.3f}' for s, w in vocab_weights[-25:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35f7d6-979c-4f58-83f3-7efde7946bf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# examine loadings of the second principal component\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "comps = pca.components_.transpose()\n",
    "vocab_weights = sorted(zip(comps[:, 1], vocab))\n",
    "\n",
    "print('Positive loadings:')\n",
    "print('\\t \\t'.join(f'{w} -> {s:.3f}' for s, w in vocab_weights[:25]))\n",
    "print(\" \")\n",
    "print('Negative loadings:')\n",
    "print('\\t \\t'.join(f'{w} -> {s:.3f}' for s, w in vocab_weights[-25:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176647f-8186-45a0-8b9f-48473567fe72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the top 50 loadings over the original PCA scatterplot\n",
    "plt.figure(figsize=(22, 15))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "\n",
    "# set colour based on file names\n",
    "colours = []\n",
    "for file in files:\n",
    "    first_letter = file[0]\n",
    "    if first_letter == 'g':\n",
    "        colours.append('limegreen')\n",
    "    elif first_letter == 'o':\n",
    "        colours.append('dodgerblue')\n",
    "    \n",
    "# label each point with an abbreviation of the file name\n",
    "for i, file in enumerate(files):\n",
    "    # take the first 10 letters of the file extension\n",
    "    abbreviated_name = file[:10] \n",
    "    plt.scatter(pca_result[i, 0], pca_result[i, 1], color=colours[i], label=file, s=50) \n",
    "    plt.text(pca_result[i, 0], pca_result[i, 1], abbreviated_name, fontsize=10)\n",
    "    \n",
    "# add top X Loadings plot\n",
    "l1, l2 = comps[:, 0], comps[:, 1]\n",
    "top_loadings = 50\n",
    "\n",
    "sorted_loadings = sorted(zip(l1, l2, vocab), key=lambda x: abs(x[0]) + abs(x[1]), reverse=True)[:top_loadings]\n",
    "for x, y, l in sorted_loadings:\n",
    "    plt.text(x, y, l, ha='center', va='center', color='grey', fontsize=8)\n",
    "    \n",
    "# add gridlines to better compare results\n",
    "plt.grid(True)\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='both', axis='both', linestyle=':', linewidth=.7)\n",
    "    \n",
    "# plot the PCA\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Analysis Loadings of Gregory vs. Original\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3994ec7-72fd-4a37-87a1-892a6dab910d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f246c26-125a-493c-bb5c-ea3820319f35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52a8e3-c986-4727-a8c0-0861b4af1e1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# WOMEN\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    # Define text names dynamically based on the number of texts\n",
    "    text_names = [f\"Text {i + 1}\" for i in range(len(texts))]\n",
    "    sentiment_scores = {name: [] for name in text_names}\n",
    "    \n",
    "    # Analyze each text separately\n",
    "    for idx, text in enumerate(texts):\n",
    "        ulster_phrases = []\n",
    "        \n",
    "        # Collect phrases or sentences containing \"Ulstermen\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in ['woman', 'women', 'wife', 'girl', 'girls', 'female', 'daughter', 'lady', 'mother', 'sister', 'fiancee', 'fiancée', \n",
    "                                'aunt', 'niece', 'girlfriend', 'queen', 'womanhood', 'princess', 'matriarch', 'damsel', 'mistress', 'heroine', 'matron',\n",
    "                               'goddess', 'hag', 'feminine', 'lass']:\n",
    "                # a small window around \"Ulster\" (5 words before and after)\n",
    "                start = max(i - 5, 0)\n",
    "                end = min(i + 6, len(words))\n",
    "                phrase = ' '.join(words[start:end])\n",
    "                ulster_phrases.append(phrase)\n",
    "        \n",
    "        # Calculate sentiment for each phrase containing \"Ulstermen\"\n",
    "        for phrase in ulster_phrases:\n",
    "            score = sia.polarity_scores(phrase)[\"compound\"]  # VADER's compound score\n",
    "            sentiment_scores[text_names[idx]].append(score)\n",
    "    \n",
    "    # Calculate average sentiment for each text collection\n",
    "    avg_sentiment = {}\n",
    "    for text_name, scores in sentiment_scores.items():\n",
    "        avg_sentiment[text_name] = np.mean(scores) if scores else 0\n",
    "    \n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da326868-0778-443d-a5c1-9b7d4d960a08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gregory\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/g_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Gregory's texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d2679-99b6-49e8-b4fb-07ff828c1ae7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/o_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Original texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5dfff-e958-4db3-a524-d26ea3c84a06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MEN\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    # Define text names dynamically based on the number of texts\n",
    "    text_names = [f\"Text {i + 1}\" for i in range(len(texts))]\n",
    "    sentiment_scores = {name: [] for name in text_names}\n",
    "    \n",
    "    # Analyze each text separately\n",
    "    for idx, text in enumerate(texts):\n",
    "        ulster_phrases = []\n",
    "        \n",
    "        # Collect phrases or sentences containing \"Ulstermen\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in ['man', 'men', 'boy', 'boys', 'boyhood', 'lad', 'husband', 'father', 'son', 'male', 'brother', 'grandfather', 'uncle', \n",
    "                                'nephew', 'hero', 'gentleman', 'guy', 'king', 'prince', 'fiance', 'fiancé', 'patriarch', 'patriarch', 'master']:\n",
    "                # 5 words before and after\n",
    "                start = max(i - 5, 0)\n",
    "                end = min(i + 6, len(words))\n",
    "                phrase = ' '.join(words[start:end])\n",
    "                ulster_phrases.append(phrase)\n",
    "        \n",
    "        # Calculate sentiment for each phrase containing \"Ulstermen\"\n",
    "        for phrase in ulster_phrases:\n",
    "            score = sia.polarity_scores(phrase)[\"compound\"]  # VADER's compound score\n",
    "            sentiment_scores[text_names[idx]].append(score)\n",
    "    \n",
    "    # Calculate average sentiment for each text collection\n",
    "    avg_sentiment = {}\n",
    "    for text_name, scores in sentiment_scores.items():\n",
    "        avg_sentiment[text_name] = np.mean(scores) if scores else 0\n",
    "    \n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681606e-fb97-4e9f-a904-fab4f357cb50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gregory\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/g_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Gregory texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469fb8-fa69-464b-9cdc-14dc84baa9cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/o_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872376b-2ff7-4a20-8a34-b39490a42c83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Original texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d79702-8b02-4bd1-bc03-c1f0517b2d6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Nationalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909928e2-68c8-4de7-b5f7-e5da07d79d41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(texts):\n",
    "    # Define text names dynamically based on the number of texts\n",
    "    text_names = [f\"Text {i + 1}\" for i in range(len(texts))]\n",
    "    sentiment_scores = {name: [] for name in text_names}\n",
    "    \n",
    "    # Analyze each text separately\n",
    "    for idx, text in enumerate(texts):\n",
    "        ulster_phrases = []  \n",
    "        \n",
    "        # Collect phrases or sentences containing \"Ulstermen\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in ['nation', 'union', 'partition', 'free', 'freedom', 'resistance', 'struggle', 'heritage', 'equal', 'rights', 'justice', \n",
    "                                'hero', 'oppression', 'struggle', 'escape', 'conflict']:\n",
    "                # a small window around \"Ulster\" (5 words before and after)\n",
    "                start = max(i - 5, 0)\n",
    "                end = min(i + 6, len(words))\n",
    "                phrase = ' '.join(words[start:end])\n",
    "                ulster_phrases.append(phrase)\n",
    "        \n",
    "        # Calculate sentiment for each phrase containing \"Ulstermen\"\n",
    "        for phrase in ulster_phrases:\n",
    "            score = sia.polarity_scores(phrase)[\"compound\"]  # VADER's compound score\n",
    "            sentiment_scores[text_names[idx]].append(score)\n",
    "    \n",
    "    # Calculate average sentiment for each text collection\n",
    "    avg_sentiment = {}\n",
    "    for text_name, scores in sentiment_scores.items():\n",
    "        avg_sentiment[text_name] = np.mean(scores) if scores else 0\n",
    "    \n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc4606-bfff-4c62-9214-51c0f9190ccd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gregory\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/g_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Gregory's texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4ed5b-a1f9-447a-80d7-cb7488b883a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/o_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Original texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833aa4e3-38f0-4941-ad87-40f99389dc73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(texts):\n",
    "    # Define text names dynamically based on the number of texts\n",
    "    text_names = [f\"Text {i + 1}\" for i in range(len(texts))]\n",
    "    sentiment_scores = {name: [] for name in text_names}\n",
    "    \n",
    "    # Analyze each text separately\n",
    "    for idx, text in enumerate(texts):\n",
    "        ulster_phrases = []\n",
    "        \n",
    "        # Collect phrases or sentences containing \"Ulstermen\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in ['north', 'northern', 'ultonian', 'ultonians', 'ultonia', 'ultonias', 'ulster', 'ulsterman', 'ulstermen', 'ulsters', 'ulstermens']:\n",
    "                # a small window around \"Ulster\" (5 words before and after)\n",
    "                start = max(i - 5, 0)\n",
    "                end = min(i + 6, len(words))\n",
    "                phrase = ' '.join(words[start:end])\n",
    "                ulster_phrases.append(phrase)\n",
    "        \n",
    "        # Calculate sentiment for each phrase containing \"Ulstermen\"\n",
    "        for phrase in ulster_phrases:\n",
    "            score = sia.polarity_scores(phrase)[\"compound\"]  # VADER's compound score\n",
    "            sentiment_scores[text_names[idx]].append(score)\n",
    "    \n",
    "    # Calculate average sentiment for each text collection\n",
    "    avg_sentiment = {}\n",
    "    for text_name, scores in sentiment_scores.items():\n",
    "        avg_sentiment[text_name] = np.mean(scores) if scores else 0\n",
    "    \n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84300b-69d1-474a-9eff-7678ebc7cdb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gregory\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/g_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Gregory's texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df87916-8629-44ed-8949-1fbbbf48e338",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/o_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Original texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e321cee-f630-489f-a85b-f4606e05890d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Folklore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0c4a4-2324-4e2d-b16c-ae8a4298699e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(texts):\n",
    "    # Define text names dynamically based on the number of texts\n",
    "    text_names = [f\"Text {i + 1}\" for i in range(len(texts))]\n",
    "    sentiment_scores = {name: [] for name in text_names}\n",
    "    \n",
    "    # Analyze each text separately\n",
    "    for idx, text in enumerate(texts):\n",
    "        ulster_phrases = []\n",
    "        \n",
    "        # Collect phrases or sentences containing \"Ulstermen\"\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in [\"sidhe\", \"fairy\", \"magic\", \"mystic\", \"pastoral\", \"tranquil\", \"ethereal\", \"timeless\", \"mystical\", \"enchanting\", \"golden age\", \"mythic\", \"otherworldly\", \"sacred\", \"humble\", \"simple\", \"harmony\", \"rustic\", \"idyllic\", \"idealized\", \"ideal\", \"picturesque\", \"romantic\", \"unspoiled\", 'folklore', 'spirit', 'supernatural', 'sorcery', 'witch', 'wizard', 'specter', 'dream', 'enchantment', 'spell', 'monster', 'myth', 'prophecy', 'legend', 'curse', 'spectre', 'ghost', 'occult', 'vision', 'charm']:\n",
    "                # a small window around \"Ulster\" (5 words before and after)\n",
    "                start = max(i - 5, 0)\n",
    "                end = min(i + 6, len(words))\n",
    "                phrase = ' '.join(words[start:end])\n",
    "                ulster_phrases.append(phrase)\n",
    "        \n",
    "        # Calculate sentiment for each phrase containing \"Ulstermen\"\n",
    "        for phrase in ulster_phrases:\n",
    "            score = sia.polarity_scores(phrase)[\"compound\"]  # VADER's compound score\n",
    "            sentiment_scores[text_names[idx]].append(score)\n",
    "    \n",
    "    # Calculate average sentiment for each text collection\n",
    "    avg_sentiment = {}\n",
    "    for text_name, scores in sentiment_scores.items():\n",
    "        avg_sentiment[text_name] = np.mean(scores) if scores else 0\n",
    "    \n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f1557-db3e-4a83-91e7-c8fc00064024",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gregory\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/g_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Gregory's texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b9712-0742-465e-8e53-47a50f2edb69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original\n",
    "folder_path = \"corpus/labelled\"\n",
    "file_paths = glob.glob(folder_path + '/o_*.txt')\n",
    "\n",
    "texts = [open(file, 'r').read() for file in file_paths]\n",
    "avg_sentiment = analyze_sentiment(texts)\n",
    "\n",
    "# Count positive and negative sentiments\n",
    "positive_count = sum(1 for score in avg_sentiment.values() if score > 0)\n",
    "negative_count = sum(1 for score in avg_sentiment.values() if score < 0)\n",
    "total_texts = len(avg_sentiment)\n",
    "\n",
    "# Calculate percentages\n",
    "positive_percentage = (positive_count / total_texts) * 100\n",
    "negative_percentage = (negative_count / total_texts) * 100\n",
    "\n",
    "# Print percentages and table header\n",
    "print(\"Original texts:\")\n",
    "print(f\"Percentage of positive sentiments: {positive_percentage:.2f}%\")\n",
    "print(f\"Percentage of negative sentiments: {negative_percentage:.2f}%\\n\")\n",
    "\n",
    "print(\"Text      Sentiment     Avg. Sentiment Score\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for text_name, sentiment in avg_sentiment.items():\n",
    "    sentiment_type = \"negative\" if sentiment < 0 else \"positive\" if sentiment > 0 else \"neutral\"\n",
    "    print(f\"{text_name:<10} {sentiment_type:<18} {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aeaad0-c114-4563-bf9a-ea1961d1e495",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1b370-d19e-425f-a032-99fcd71a681b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize empty list to hold all sentences\n",
    "texts = []\n",
    "\n",
    "# Read and process the tokenized files\n",
    "file_paths = [\"corpus/tokenized_g.txt\", \"corpus/tokenized_o.txt\"]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  # split on whitespace if already tokenized\n",
    "            texts.append(tokens)  # Append the list of tokens to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66170174-1245-4685-87b5-bb96abba245d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model on the collected tokenized texts\n",
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeda176-7560-422d-9616-12b4b54d0088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de1478-af41-4c65-af0f-b93807f7ae02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# WOMAN\n",
    "\n",
    "# Calculate reference embedding for keywords\n",
    "woman_keywords = ['woman', 'women', 'wife', 'girl', 'girls', 'female', 'daughter', 'lady', 'mother', 'sister', 'aunt', 'niece', 'girlfriend', 'queen', 'womanhood', 'princess', 'matriarch', 'damsel', 'matron', 'goddess', 'hag', 'feminine', 'lass']\n",
    "\n",
    "# Filter out keywords not present in the Word2Vec vocabulary\n",
    "filtered_keywords = [keyword for keyword in woman_keywords if keyword in word2vec_model.wv]\n",
    "print(\"These are the keywords that are present in the texts:\")\n",
    "print(filtered_keywords)\n",
    "\n",
    "if filtered_keywords:  \n",
    "    keywords_embedding = np.mean([word2vec_model.wv[keyword] for keyword in filtered_keywords], axis=0)\n",
    "\n",
    "    # Calculate text embeddings and similarity scores\n",
    "    similarity_scores = []\n",
    "    for text in texts:\n",
    "        # Get embeddings for words in the text that are present in the model\n",
    "        word_embeddings = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
    "        \n",
    "        if word_embeddings:  # Only calculate if there are words present in the vocabulary\n",
    "            text_embedding = np.mean(word_embeddings, axis=0)\n",
    "            similarity_score = cosine_similarity([text_embedding], [keywords_embedding])[0][0]\n",
    "            similarity_scores.append(similarity_score)\n",
    "        else:\n",
    "            similarity_scores.append(0)  # Assign 0 if no words are in the vocabulary\n",
    "    \n",
    "    print(\"\\nSimilarity Scores for female-related Keywords:\")\n",
    "    print(\"Gregory vs. Original text\")\n",
    "    print(similarity_scores)\n",
    "\n",
    "\n",
    "# Determine the text with highest similarity score\n",
    "highest_similarity = np.argmax(similarity_scores)\n",
    "high_sim_text = texts[highest_similarity]\n",
    "\n",
    "text_names = [\"Gregory's texts\", \"Original texts\"]\n",
    "most_similar_text_name = text_names[highest_similarity]\n",
    "print(f\"\\n{most_similar_text_name} has the highest similarity to the female theme.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736fae5-d829-4a8b-a131-e154a9750a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MAN\n",
    "\n",
    "# Calculate reference embedding for keywords\n",
    "man_keywords = ['man', 'men', 'boy', 'boys', 'boyhood', 'lad', 'husband', 'father', 'son', 'male', 'brother', 'grandfather', 'uncle', 'nephew', 'gentleman', 'guy', 'king', 'prince', 'patriarch', 'patriarch']\n",
    "\n",
    "# Filter out keywords not present in the Word2Vec vocabulary\n",
    "filtered_keywords = [keyword for keyword in man_keywords if keyword in word2vec_model.wv]\n",
    "print(\"These are the keywords that are present in the texts:\")\n",
    "print(filtered_keywords)\n",
    "\n",
    "if filtered_keywords:  \n",
    "    keywords_embedding = np.mean([word2vec_model.wv[keyword] for keyword in filtered_keywords], axis=0)\n",
    "\n",
    "    # Calculate text embeddings and similarity scores\n",
    "    similarity_scores = []\n",
    "    for text in texts:\n",
    "        # Get embeddings for words in the text that are present in the model\n",
    "        word_embeddings = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
    "        \n",
    "        if word_embeddings:  # Only calculate if there are words present in the vocabulary\n",
    "            text_embedding = np.mean(word_embeddings, axis=0)\n",
    "            similarity_score = cosine_similarity([text_embedding], [keywords_embedding])[0][0]\n",
    "            similarity_scores.append(similarity_score)\n",
    "        else:\n",
    "            similarity_scores.append(0)  # Assign 0 if no words are in the vocabulary\n",
    "    \n",
    "    print(\"\\nSimilarity Scores for male-related Keywords:\")\n",
    "    print(\"Gregory vs. Original text\")\n",
    "    print(similarity_scores)\n",
    "\n",
    "\n",
    "# Determine the text with highest similarity score\n",
    "highest_similarity = np.argmax(similarity_scores)\n",
    "high_sim_text = texts[highest_similarity]\n",
    "\n",
    "text_names = [\"Gregory's texts\", \"Original texts\"]\n",
    "most_similar_text_name = text_names[highest_similarity]\n",
    "print(f\"\\n{most_similar_text_name} has the highest similarity to the male theme.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55367bd3-f039-4a3d-8839-8b4ac839c918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Nationalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c0d29-b54e-40db-901d-7e658f118bdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate reference embedding for keywords\n",
    "nationalism_keywords = ['nation', 'union', 'partition', 'free', 'freedom', 'resistance', 'struggle', 'heritage', 'equal', 'rights', 'justice', \n",
    "                                'hero', 'oppression', 'struggle', 'escape', 'conflict']\n",
    "\n",
    "# Filter out keywords not present in the Word2Vec vocabulary\n",
    "filtered_keywords = [keyword for keyword in nationalism_keywords if keyword in word2vec_model.wv]\n",
    "print(\"These are the keywords that are present in the texts:\")\n",
    "print(filtered_keywords)\n",
    "\n",
    "if filtered_keywords:  \n",
    "    keywords_embedding = np.mean([word2vec_model.wv[keyword] for keyword in filtered_keywords], axis=0)\n",
    "\n",
    "    # Calculate text embeddings and similarity scores\n",
    "    similarity_scores = []\n",
    "    for text in texts:\n",
    "        # Get embeddings for words in the text that are present in the model\n",
    "        word_embeddings = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
    "        \n",
    "        if word_embeddings:  # Only calculate if there are words present in the vocabulary\n",
    "            text_embedding = np.mean(word_embeddings, axis=0)\n",
    "            similarity_score = cosine_similarity([text_embedding], [keywords_embedding])[0][0]\n",
    "            similarity_scores.append(similarity_score)\n",
    "        else:\n",
    "            similarity_scores.append(0)  # Assign 0 if no words are in the vocabulary\n",
    "    \n",
    "    print(\"\\nSimilarity Scores for nationalism-related Keywords:\")\n",
    "    print(\"Gregory vs. Original text\")\n",
    "    print(similarity_scores)\n",
    "\n",
    "\n",
    "# Determine the text with highest similarity score\n",
    "highest_similarity = np.argmax(similarity_scores)\n",
    "high_sim_text = texts[highest_similarity]\n",
    "\n",
    "text_names = [\"Gregory's texts\", \"Original texts\"]\n",
    "most_similar_text_name = text_names[highest_similarity]\n",
    "print(f\"\\n{most_similar_text_name} has the highest similarity to the nationalism theme.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e92081-a5eb-46e3-aef1-e913d0d179ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Folklore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a7498-9d43-41d3-8d51-c7278ce1fa03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate reference embedding for keywords\n",
    "folklore_keywords = [\"sidhe\", \"fairy\", \"magic\", \"mystic\", \"pastoral\", \"tranquil\", \"ethereal\", \"timeless\", \"mystical\", \"enchanting\", \"golden age\", \"mythic\", \"otherworldly\", \"sacred\", \"humble\", \"simple\", \"harmony\", \"rustic\", \"idyllic\", \"idealized\", \"ideal\", \"picturesque\", \"romantic\", \"unspoiled\", 'folklore', 'spirit', 'supernatural', 'sorcery', 'witch', 'wizard', 'specter', 'dream', 'enchantment', 'spell', 'monster', 'myth', 'prophecy', 'legend', 'curse', 'spectre', 'ghost', 'occult', 'vision', 'charm']\n",
    "\n",
    "# Filter out keywords not present in the Word2Vec vocabulary\n",
    "filtered_keywords = [keyword for keyword in folklore_keywords if keyword in word2vec_model.wv]\n",
    "print(\"These are the keywords that are present in the texts:\")\n",
    "print(filtered_keywords)\n",
    "\n",
    "if filtered_keywords:  \n",
    "    keywords_embedding = np.mean([word2vec_model.wv[keyword] for keyword in filtered_keywords], axis=0)\n",
    "\n",
    "    # Calculate text embeddings and similarity scores\n",
    "    similarity_scores = []\n",
    "    for text in texts:\n",
    "        # Get embeddings for words in the text that are present in the model\n",
    "        word_embeddings = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv]\n",
    "        \n",
    "        if word_embeddings:  # Only calculate if there are words present in the vocabulary\n",
    "            text_embedding = np.mean(word_embeddings, axis=0)\n",
    "            similarity_score = cosine_similarity([text_embedding], [keywords_embedding])[0][0]\n",
    "            similarity_scores.append(similarity_score)\n",
    "        else:\n",
    "            similarity_scores.append(0)  # Assign 0 if no words are in the vocabulary\n",
    "    \n",
    "    print(\"\\nSimilarity Scores for folklore-related Keywords:\")\n",
    "    print(\"Gregory vs. Original text\")\n",
    "    print(similarity_scores)\n",
    "\n",
    "\n",
    "# Determine the text with highest similarity score\n",
    "highest_similarity = np.argmax(similarity_scores)\n",
    "high_sim_text = texts[highest_similarity]\n",
    "\n",
    "text_names = [\"Gregory's texts\", \"Original texts\"]\n",
    "most_similar_text_name = text_names[highest_similarity]\n",
    "print(f\"\\n{most_similar_text_name} has the highest similarity to the folklore theme.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3c002-8015-45d6-87f7-375872f9b45a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove archaic words found in original texts (\"thy\", \"thee\", etc.), character names, and place names except \"Ireland\" and it's variants \n",
    "all_names = ['said', 'Táin bo Cuailgne', 'crachain ', 'ferceirtne', 'etercels', 'domnand', 'ferbeson', 'craiftine', 'beara', 'aife', 'conaing', 'conlaoch', 'culain', 'crandce', 'deride', 'feochar', 'cerdda', 'chula', 'comainm', 'feoraind', 'culands', 'duma', 'cormac', 'connad', 'eterscele', 'forgaimin', 'conrach', 'cerna', 'ferb', 'eruaine', 'bunne', 'ferchon', 'donegal', 'deroil', 'fer', 'forais', 'aife', 'chulainn', 'ferdach', 'fn', 'culann', 'fun', 'feradach', 'foras', 'aife', 'clath', 'chulainmany', 'culend', 'dundalk', 'fngin', 'banba', 'bithln', 'conroi', 'coscra', 'ferde', 'ethal', 'foraoi', 'ferbend', 'comchraid', 'connall', 'cru', 'dunfor', 'ad', 'agnomon', 'brecc', 'fain', 'banba', 'aife', 'ailell', 'ailells', 'ailill', 'ainnle', 'alban', 'amergin', 'anbual', 'angus', 'aoife', 'ardan', 'art', 'assaroe', 'atha', 'athach', 'athairne', 'athanforaire', 'athcoltna', 'athgowla', 'athisech', 'bachlach', 'banba', 'banbach', 'banchuig', 'banchuing', 'bel', 'bithln', 'blai', 'blaicne', 'blaitmic', 'blathnat', 'boann', 'bodb', 'boinne', 'brecc', 'breg', 'bregia', 'bregians', 'bregoes', 'bregons', 'bregros', 'bregs', 'bren', 'brenguir', 'breoga', 'breogann', 'bres', 'bresal', 'bresel', 'breslech', 'breslige', 'brian', 'bricriu', 'brod', 'buadach', 'byres', 'caer', 'caerthund', 'cailnge', 'caladcholg', 'calatin', 'calatins', 'canchuig', 'cathbad', 'cearnach', 'cecht', 'celthair', 'celthairs', 'cernach', 'chetaig', 'chetail', 'chilair', 'chille', 'chliss', 'chon', 'chrachan', 'chrachna', 'chulain', 'chulainn', 'chulainns', 'chulainnwise', 'chulalnns', 'ciallglind', 'cinn', 'clath', 'clothach', 'cona', 'conachar', 'conachars', 'conail', 'conaille', 'conaire', 'conall', 'conchobar', 'conchobars', 'conchobor', 'conchubar', 'conchubars', 'condere', 'conlaoch', 'conloingeas', 'connacht', 'connaght', 'connaid', 'connall', 'connaught', 'connaughts', 'connla', 'connud', 'conor', 'conors', 'conrach', 'corco', 'cork', 'cormac', \"cormac's\", 'cormacs', 'cornkiln', 'couch', 'cr', 'crachain', 'crachna', 'crachu', 'craiftine', 'crandce', 'crannach', 'crantail', 'crantails', 'crfoit', 'crft', 'criadh', 'crichid', 'crn', 'cro', 'cruachan', 'cruachs', 'cruahan', 'cruaidin', 'cruife', 'cruifne', 'cscraid', 'cu', 'cuailgne', 'cualgne', 'cuchulain', 'cuchulaind', 'cuchulainn', 'cuchulains', 'cuchullin', 'cuillenn', 'culain', 'culann', 'curad', 'curoi', 'dagda', 'daire', 'dalach', 'damach', 'daman', 'damin', 'danaan', 'darius', 'dechtire', 'deidre', 'deirdre', 'delgan', 'dessa', 'diad', 'diman', 'donall', 'donnell', 'dost', 'draccon', 'drimne', 'dris', 'driscoll', 'drucht', 'dundealgan', 'durthacht', 'ed', 'edond', 'egem', 'ehmain', 'eidre', 'eirr', 'eirrgi', 'eisi', 'eit', 'eitche', 'eithlend', ':', 'Edinburgh MS', 'eithlin', 'eithne', 'ele', 'elga', 'ellann', 'ellonn', 'emain', 'eman', 'emania', 'emer', 'emers', 'emna', 'enchenn', 'eogan', 'eoghan', 'eterscel', 'ethal', 'ethgowla', 'ethne', 'fachaig', 'fachtna', 'fachtnas', 'faindle', 'fainnle', 'fand', 'fedelmid', 'fedlimid', 'fer', 'ferb', 'ferdiad', 'ferg', 'fergel', 'ferger', 'fergne', 'fergus', 'ferloga', 'ferobain', 'ferogain', 'fiannamail', 'filliud', 'findabair', 'findabairs', 'findach', 'findairget', 'findbennach', 'findchad', 'findchoem', 'findchoimi', 'findderg', 'findomain', 'findruine', 'fingalach', 'fingan', 'fingin', 'finishthe', 'finn', 'finnabair', 'finnabar', 'finncairn', 'finnchairn', 'finnched', 'finnchoem', 'fintans', 'firb', 'firecearna', 'firgein', 'fngin', 'fomor', 'forbuide', 'forcul', 'fordofthemrrigan', 'fordui', 'forgaimin', 'forgall', 'fornuil', 'foscul', 'fosse', 'fota', 'fraech', 'fraidtine', 'frechlethain', 'frecul', 'fretted', 'fri', 'friuch', 'furachar', 'furbaidewhite', 'furbuithe', 'geanann', 'genann', 'gerg', 'gergs', 'hast', 'hath', 'ibar', 'ie', 'ilsuanach', 'ingcel', 'ini', 'iollan', 'iubar', 'labra', 'labraid', 'laeg', 'laegaire', 'laegaires', 'laegh', 'lecca', 'lenister', 'lethan', 'levarcham', 'liban', 'loigaire', 'lomna', 'lugai', 'lugaid', 'mac', 'macha', 'mache', 'maev', 'maeve', 'maeves', 'magh', 'maine', 'manannan', 'mane', 'mani', 'meave', 'medb', 'meix', 'muirthemne', 'munster', 'mve', 'naisi', 'naoise', 'nechtain', 'nechtan', 'nera', 'ness', 'niafer', 'niall', 'niamh', 'oengus', 'quoth', 'rogain', 'rogh', 'roig', 'scathach', 'setanta', 'shalt', 'sid', 'slebe', 'sliab fuait', 'slieve', 'sualtaim', 'sualtim', 'tata', 'teamhair', 'thee', 'thine', 'thou', 'thus', 'thy', 'tis', 'ulster', 'usnach', 'usnas', 'viz', 'wilt', 'ye']\n",
    "\n",
    "# remove duplicates\n",
    "names = set(all_names)\n",
    "print(f\"Unique names: {len(names)} total\")\n",
    "\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(name) for name in names) + r')\\b', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128174b-08f8-42d2-8dba-146919163d26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model on the collected tokenized texts\n",
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b292ab-4405-4417-ab5a-11382ed675c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"corpus/labelled\"\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "corpus = []\n",
    "for file in files:\n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        # Replace matched names with an empty string\n",
    "        cleaned_text = pattern.sub(\"\", text)\n",
    "        corpus.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfa510-4601-4483-8ca0-fdc58a3664f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix\n",
    "stop_words_list = list(stop_words)  \n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words_list)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate mean TF-IDF for each term across the entire corpus\n",
    "tfidf_means = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "\n",
    "# Get top terms\n",
    "top_n = 50\n",
    "top_terms_indices = np.argsort(tfidf_means)[-top_n:]\n",
    "top_terms = [vocab[i] for i in top_terms_indices]\n",
    "top_tfidf_matrix = tfidf_matrix[:, top_terms_indices].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e6b68-18b9-4fc3-be8a-9441c0622be4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_terms_scores = [tfidf_means[i] for i in top_terms_indices]\n",
    "\n",
    "# Print the top terms with their average TF-IDF scores -> these are the most unique and defining words across all texts\n",
    "# They appear more requently in specific documents while being relatively rare across the entire corpus.\n",
    "print(\"Top terms with the highest average TF-IDF scores across the corpus:\")\n",
    "for term, score in zip(top_terms, top_terms_scores):\n",
    "    print(f\"{term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01be66-76de-4272-94d7-b548f1aa63df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd408f0-cc32-447f-834e-d65ae73b8ee2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filter top terms (keep only valid keywords pressent in the model's vocab)\n",
    "valid_keywords = [term for term in top_terms if term in model.wv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41182f01-d438-4a42-8515-edf8f2634b97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyse the contextual similarity of each keyword\n",
    "for keyword in valid_keywords:\n",
    "    similar_words = model.wv.most_similar(keyword, topn=10)\n",
    "    print(f\"\\n Top 10 words similar to '{keyword}':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  {word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd3569-6a27-40d6-8aec-4efb10f0682c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# analyse the contextual similarity with a PCA\n",
    "# Get embeddings for the valid keywords\n",
    "keyword_embeddings = [model.wv[keyword] for keyword in valid_keywords]\n",
    "\n",
    "# Reduce dimensionality to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(keyword_embeddings)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, keyword in enumerate(valid_keywords):\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "    plt.text(reduced_embeddings[i, 0] + 0.0001, reduced_embeddings[i, 1] + 0.0001, keyword, fontsize=9)\n",
    "plt.title(\"Word2Vec Embeddings for Keywords\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6666ee7-e0de-463c-9836-d33d580bd72c",
   "metadata": {},
   "source": [
    "## Collocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e4432-09bd-4a58-aa91-07a43f4ef524",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    # Remove punctuation and numbers (only keep alphabetic characters and spaces)\n",
    "    word = re.sub(r'[^a-zA-Z\\s]', '', word)  \n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3191014-be43-45e2-a396-5f1556fa0d47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the paths for the two specific files\n",
    "file_paths = ['corpus/all_gregory.txt', 'corpus/all_original.txt']\n",
    "texts = []\n",
    "for file in file_paths:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "# Convert all words in each text to lowercase to ensure consistency\n",
    "texts = [[clean_word(word) for word in text.split()] for text in texts]\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Text names for the two files\n",
    "text_names = [\"Gregory's texts\", \"Original texts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95cfd62-7781-4294-82e8-b112c3942715",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19db49c-5f05-4164-9694-4b9d87801fa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WOMAN\n",
    "\n",
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'woman':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afa4be-b3ff-4330-8c20-c084229940d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WOMAN\n",
    "\n",
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'women':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cd7a6-7e9e-4647-9014-7e22a1e19273",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAN\n",
    "\n",
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'man':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2d4c5-c4c8-4f11-8023-53a7ab329776",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WOMAN\n",
    "\n",
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'men':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfb281-0c14-41ef-be48-4f8099f6da75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Nationalism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe781c-f892-4186-8e6d-3161859cef49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'ulster':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822220a9-5c41-4bea-a9d5-1784f7a28629",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'hero':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f30fa-6e13-4bc7-a32b-c280f93f6002",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'royal':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ac6ea-f73a-4ea9-ace4-33d1afec4e89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Folklore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a6556-bc86-493e-b998-40723f09bd4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'love':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487c098-4eab-49ae-abb2-0e071b0ec3a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze each text separately\n",
    "for idx, text in enumerate(texts):\n",
    "    before_words = []\n",
    "    after_words = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for i, word in enumerate(text):\n",
    "        if word == 'chief':  \n",
    "            # Get the word before the keyword if it exists and is not a stopword\n",
    "            if i > 0 and text[i - 1] not in stop_words:\n",
    "                before_words.append(text[i - 1])\n",
    "            # Get the word after the keyword if it exists and is not a stopword\n",
    "            if i < len(text) - 1 and text[i + 1] not in stop_words:\n",
    "                after_words.append(text[i + 1])\n",
    "\n",
    "    # Count the frequency of words before and after the keyword in this text\n",
    "    before_counter = Counter(before_words)\n",
    "    after_counter = Counter(after_words)\n",
    "\n",
    "    # Get the top 10 most common words before and after the keyword in this text\n",
    "    top_before = before_counter.most_common(10)\n",
    "    top_after = after_counter.most_common(10)\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    results[text_names[idx]] = {\n",
    "        \"Top 10 words before the keyword\": top_before,\n",
    "        \"Top 10 words after the keyword\": top_after\n",
    "    }\n",
    "\n",
    "# Print the results\n",
    "for text_name, result in results.items():\n",
    "    print(f\"\\nResults for {text_name}:\")\n",
    "    print(\"Top 10 words before the keyword:\")\n",
    "    for word, count in result[\"Top 10 words before the keyword\"]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words after the keyword:\")\n",
    "    for word, count in result[\"Top 10 words after the keyword\"]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21c4a4-fc73-4122-827a-d300ca99b1cd",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef40a8-eeb8-4de2-8abe-513a7ea45feb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define base parth for text files\n",
    "base_path = r\"D:\\Project\\corpus"\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"links.xlsx\"\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Extract file links from the \"Links\" sheet\n",
    "links_df = sheets_dict[\"Links\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e67ffa-83e8-448a-abaa-1ec58f75a303",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to load text files\n",
    "def load_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().lower()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Function to align corpora\n",
    "def align_corpora(corpus_g, corpus_o, alignment_map):\n",
    "    for pattern, replacement in alignment_map.items():\n",
    "        regex_pattern = r'\\b(?:' + pattern.replace(\"|\", \"|\") + r')\\b'  # Whole word match\n",
    "        corpus_g = re.sub(regex_pattern, replacement, corpus_g)\n",
    "        corpus_o = re.sub(regex_pattern, replacement, corpus_o)\n",
    "    return corpus_g, corpus_o\n",
    "\n",
    "# Function to extract character interactions\n",
    "def extract_interactions(tokens, characters, window_size=20):\n",
    "    interactions = defaultdict(int)\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i + window_size]\n",
    "        present_chars = [char for char in characters if char in window]\n",
    "        for c1, c2 in set([(x, y) for x in present_chars for y in present_chars if x != y]):\n",
    "            interactions[tuple(sorted([c1, c2]))] += 1\n",
    "    return interactions\n",
    "\n",
    "# Function to compute and print network metrics\n",
    "def compute_network_metrics(interactions, title, corpus):\n",
    "    G = nx.Graph()\n",
    "    for (char1, char2), weight in interactions.items():\n",
    "        G.add_edge(char1, char2, weight=weight)\n",
    "\n",
    "    metrics = {\n",
    "        \"Number of Nodes\": G.number_of_nodes(),\n",
    "        \"Number of Edges\": G.number_of_edges(),\n",
    "        \"Density\": nx.density(G),\n",
    "        \"Average Degree\": sum(dict(G.degree()).values()) / G.number_of_nodes() if G.number_of_nodes() > 0 else 0,\n",
    "        \"Degree Centrality\": nx.degree_centrality(G),\n",
    "        \"Betweenness Centrality\": nx.betweenness_centrality(G),\n",
    "        \"Closeness Centrality\": nx.closeness_centrality(G),\n",
    "        \"Eigenvector Centrality\": nx.eigenvector_centrality(G) if G.number_of_nodes() > 0 else {},\n",
    "    }\n",
    "\n",
    "    print(f\"\\n\\nMetrics for {title}\")\n",
    "    print(f\"Corpus Title ({title})\")  # Print a sample of the corpus\n",
    "    print(f\"____________________________________________________________\")  # Print a sample of the corpus\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"{key}: {sorted(value.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "# Function to visualize character networks\n",
    "def visualize_network(interactions, title):\n",
    "    G = nx.Graph()\n",
    "    for (char1, char2), weight in interactions.items():\n",
    "        G.add_edge(char1, char2, weight=weight)\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in G.edges(data=True):\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "    \n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    node_x, node_y, node_text = [], [], []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_text.append(f\"{node} ({G.degree[node]})\")\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=node_text,\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=[G.degree[n] * 5 for n in G.nodes()], color=\"blue\", opacity=0.7),\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=title,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=0, l=0, r=0, t=40),\n",
    "                        xaxis=dict(showgrid=False, zeroline=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False)\n",
    "                    ))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4e26d-265a-48b9-a611-4c60a39f9ba5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over all Name of Pairs in the Links file\n",
    "for _, row in links_df.iterrows():\n",
    "    name_pair = row[\"Name of Pair\"]\n",
    "    file_g = os.path.join(base_path, row[\"Retelling Text Link\"])\n",
    "    file_o = os.path.join(base_path, row[\"Original Text Link\"])\n",
    "\n",
    "    # Skip if no alignment sheet exists\n",
    "    if name_pair not in sheets_dict:\n",
    "        print(f\"Skipping {name_pair} (No alignment sheet found)\")\n",
    "        continue\n",
    "\n",
    "    corpus_g, corpus_o = load_text(file_g), load_text(file_o)\n",
    "    alignment_df = sheets_dict[name_pair]\n",
    "    alignment_map = dict(zip(alignment_df[\"Variation 1\"].astype(str) + \"|\" + alignment_df[\"variation 2\"].astype(str),\n",
    "                              alignment_df[\"Aligned Name\"]))\n",
    "    corpus_g, corpus_o = align_corpora(corpus_g, corpus_o, alignment_map)\n",
    "    \n",
    "    tokens_g, tokens_o = preprocess_text(corpus_g), preprocess_text(corpus_o)\n",
    "    characters = list(set(alignment_map.values()))\n",
    "    interactions_g, interactions_o = extract_interactions(tokens_g, characters), extract_interactions(tokens_o, characters)\n",
    "    \n",
    "    print(f\"Processing: {name_pair}\")\n",
    "    compute_network_metrics(interactions_g, f\"Retelling: {name_pair}\", corpus_g)\n",
    "    compute_network_metrics(interactions_o, f\"Original: {name_pair}\", corpus_o)\n",
    "    visualize_network(interactions_g, f\"Character Network - Retelling: {name_pair}\")\n",
    "    visualize_network(interactions_o, f\"Character Network - Original: {name_pair}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
